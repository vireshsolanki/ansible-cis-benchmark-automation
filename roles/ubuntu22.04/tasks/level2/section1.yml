###1.1.1.x Ensure  mounting 
---
- name: "PRELIM | AUDIT | Set default values for facts"
  ansible.builtin.set_fact:
      control_1_6_1_4_was_run: false
      ubtu22cis_apparmor_enforce_only: false
  changed_when: false

- name: "PRELIM | AUDIT | Register if snap being used"
  ansible.builtin.shell: df -h | grep -wc "/snap"
  changed_when: false
  failed_when: snap_pkg_mgr.rc not in [ 0, 1 ]
  register: snap_pkg_mgr
  tags:
      - rule_1.1.1.2
      - always
      - ubtu22cis_level2
  when:
      - ubtu22cis_rule_1_1_1_2

- name: "PRELIM | AUDIT | Register if squashfs is built into the kernel"
  ansible.builtin.shell: cat /lib/modules/$(uname -r)/modules.builtin | grep  -c "squashfs"
  changed_when: false
  failed_when: squashfs_builtin.rc not in [ 0, 1 ]
  register: squashfs_builtin
  tags:
      - rule_1.1.1.2
      - always
      - ubtu22cis_level2
  when:
      - ubtu22cis_rule_1_1_1_2

- name: "1.1.1.2 | PATCH | Ensure mounting of squashfs filesystems is disabled"
  block:
      - name: "1.1.1.2 | PATCH | Ensure mounting of squashfs filesystems is disabled | Edit modprobe config"
        ansible.builtin.lineinfile:
            dest: /etc/modprobe.d/squashfs.conf
            regexp: '^(#)?install squashfs(\\s|$)'
            line: install squashfs /bin/true
            create: true

      - name: "1.1.1.2 | PATCH | Ensure mounting of squashfs filesystems is disabled | blacklist"
        ansible.builtin.lineinfile:
            path: /etc/modprobe.d/blacklist.conf
            regexp: "^(#)?blacklist squashfs(\\s|$)"
            line: "blacklist squashfs"
            create: true
            mode: '0600'

      - name: "1.1.1.2 | PATCH | Ensure mounting of squashfs filesystems is disabled | Disable squashfs"
        community.general.modprobe:
            name: squashfs
            state: absent
        when: ansible_connection != 'docker'
  notify: Update_Initramfs
  when:
      - ubtu22cis_rule_1_1_1_2
      - snap_pkg_mgr.stdout == "0"
      - squashfs_builtin.stdout == "0"
  tags:
      - level2-server
      - level2-workstation
      - ubtu22cis_level2
      - automated
      - patch
      - rule_1.1.1.2
      - squashfs

- name: "1.1.1.3 | PATCH | Ensure mounting of udf filesystems is disabled"
  block:
      - name: "1.1.1.3 | PATCH | Ensure mounting of udf filesystems is disabled | Edit modprobe config"
        ansible.builtin.lineinfile:
            dest: /etc/modprobe.d/udf.conf
            regexp: '^(#)?install udf(\\s|$)'
            line: install udf /bin/true
            create: true

      - name: "1.1.1.3 | PATCH | Ensure mounting of udf filesystems is disabled | blacklist"
        ansible.builtin.lineinfile:
            path: /etc/modprobe.d/blacklist.conf
            regexp: "^(#)?blacklist udf(\\s|$)"
            line: "blacklist udf"
            create: true
            mode: '0600'

      - name: "1.1.1.3 | PATCH | Ensure mounting of udf filesystems is disabled | Disable udf"
        community.general.modprobe:
            name: udf
            state: absent
        when: ansible_connection != 'docker'
  notify: Update_Initramfs
  when:
      - ubtu22cis_rule_1_1_1_3
  tags:
      - level2-server
      - ubtu22cis_level2
      - level2-workstation
      - automated
      - patch
      - rule_1.1.1.3
      - udf
- name: "1.1.6.1 | AUDIT | Ensure /var/log/audit is a separate partition"
  block:
      - name: "1.1.6.1 | AUDIT | Ensure /var/log/audit is a separate partition | Absent"
        ansible.builtin.debug:
            msg: "Warning!! {{ required_mount }} doesn't exist. This is a manual task"

      - name: "1.1.6.1 | WARN | Ensure /var/log/audit is a separate partition | warn_count"
        ansible.builtin.import_tasks:
            file: warning_facts.yml
  vars:
      warn_control_id: '1.1.6.1'
      required_mount: '/var/log/audit'
  when:
      - required_mount not in mount_names
      - ubtu22cis_rule_1_1_6_1
  tags:
      - level2-server
      - ubtu22cis_level2
      - level2-workstation
      - automated
      - audit
      - rule_1.1.6.1
      - varlogaudit

###1.8.1 Ensure Gnome is removed
- name: "1.8.1 | PATCH | Ensure GNOME Display Manager is removed"
  ansible.builtin.package:
      name: gdm3
      state: absent
  when:
      - ubtu22cis_rule_1_8_1
      - not ubtu22cis_desktop_required
      - ubtu22cis_disruption_high
      - "'gdm3' in ansible_facts.packages"
  tags:
      - level2-server
      - ubtu22cis_level2
      - manual
      - patch
      - rule_1.8.1
      - gnome


# Controls 1.6.1.4 and 1.6.1.3 target the same setting and thus should not be run together.
# Because control 1.6.1.4 is stricter than 1.6.1.3, we need to change the order --
# control 1.6.1.4 then registers the fact that is has run and thus keeps 1.6.1.3 from running.

- name: "1.6.1.4 | PATCH | Ensure all AppArmor Profiles are enforcing"
  block:
      - name: "1.6.1.4 | PATCH | Ensure all AppArmor Profiles are enforcing | Make sure that 1.6.1.3 is not run"
        ansible.builtin.set_fact:
            control_1_6_1_4_was_run: true
            ubtu22cis_apparmor_enforce_only: true
        changed_when: false

      - name: "1.6.1.4 | PATCH | Ensure all AppArmor Profiles are enforcing | Get pre apply enforce count"
        ansible.builtin.shell: apparmor_status |  grep "profiles are in enforce mode" | tr -d -c 0-9
        changed_when: false
        failed_when: false
        register: ubtu22cis_1_6_1_4_pre_count

      - name: "1.6.1.4 | PATCH | Ensure all AppArmor Profiles are enforcing | Apply enforcing to /etc/apparmor.d profiles"
        ansible.builtin.shell: aa-enforce /etc/apparmor.d/*
        changed_when: false
        failed_when: false

      - name: "1.6.1.4 | PATCH | Ensure all AppArmor Profiles are enforcing | Get post apply enforce count"
        ansible.builtin.shell: apparmor_status |  grep "profiles are in enforce mode" | tr -d -c 0-9
        changed_when: false
        failed_when: false
        register: ubtu22cis_1_6_1_4_post_count

      - name: "1.6.1.4 | PATCH | Ensure all AppArmor Profiles are enforcing | This flags for idempotency"
        ansible.builtin.debug:
            msg: Changed! The profiles in /etc/apparmor.d were set to enforcing
        changed_when: true
        when: ubtu22cis_1_6_1_4_pre_count.stdout != ubtu22cis_1_6_1_4_post_count.stdout
  when:
      - ubtu22cis_rule_1_6_1_4
      - not ubtu22cis_apparmor_disable
  tags:
      - level2-server
      - level2-workstation
      - ubtu22cis_level2
      - automated
      - scored
      - patch
      - rule_1.6.1.4
      - apparmor

### 1.1.x.x Ensure separate partition exists for directories


- name: "1.1.5.5  | AUDIT | Ensure /var/log/audit is a separate partition"
  block:
      - name: "1.1.5.5 | AUDIT | Ensure /var/log/audit is a separate partition | Absent"
        ansible.builtin.debug:
            msg: "Warning!! {{ required_mount }} doesn't exist. This is a manual task"

      - name: "1.1.5.5 | WARN | Ensure /var/log/audit is a separate partition | warn_count"
        ansible.builtin.import_tasks:
            file: warning_facts.yml
  vars:
      warn_control_id: '1.1.5.5'
      required_mount: '/var/log/audit'
  when:
      - required_mount not in mount_names
      - ubtu22cis_rule_1_1_5_5
  tags:
      - level2-server
      - level2-workstation
      - ubtu22cis_level2
      - automated
      - audit
      - rule_1.1.5.5
      - varlogaudit
- name: "1.1.5.1 | AUDIT | Ensure /var/log is a separate partition"
  block:
      - name: "1.1.5.1 | AUDIT | Ensure /var/log is a separate partition | Absent"
        ansible.builtin.debug:
            msg: "Warning!! {{ required_mount }} doesn't exist. This is a manual task"

      - name: "1.1.5.1 | WARN | Ensure /var/log is a separate partition | warn_count"
        ansible.builtin.import_tasks:
            file: warning_facts.yml
  vars:
      warn_control_id: '1.1.5.1'
      required_mount: '/var/log'
  when:
      - required_mount not in mount_names
      - ubtu22cis_rule_1_1_5_1
  tags:
      - level2-server
      - level2-workstation
      - ubtu22cis_level2
      - automated
      - audit
      - rule_1.1.5.1
      - varlog



###


  


  
